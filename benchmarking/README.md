# Benchmarking overview

The benchmark results are located in `benchmark_results` with a subdirectory for each dataset. These results are generated by the equivalently named python scripts `<dataset>.py`. Each script is styled in a similar pattern, consisting of:

- The DataManager definition which specified the train, val, test splits along with the initial labelled and unlabelled indices in the queryable (train) pool.
- A `trial` function which specifies the manner in which the PyRelationAL pipeline is constructed. Done so as we may want different model_managers, oracles, seeds, etc. to be run
- Settings for names, and resources to be used for the trials.

Depending on whether the model task is classification or regression, each script will use the relevant utilities from the `classification_experiment_utils.py` or `regression_experiment_utils.py` modules respectively.

We leverage Ray Tune's job scheduling to efficiently distribute and manage the running of the jobs. Each job creates an individual "trial" with it's results and other logs sent `benchmark_results/<dataset>/`. After this we can collate and analyse the trials at the trial or whole experiment level by reading these results files into a `result_grid` (see `results_grid_analysis.ipynb` for examples.)


# TL;DR

Run `<dataset.py>`, adjust and check benchmarks in `results_grid_analysis.ipynb`


# TODOs

- Fix random strategy, under the current implementation of the pipeline run the model is not trained between rounds (in other strategies the model is trained as part of the strategy). This causes the performance to not improve until the very end.
