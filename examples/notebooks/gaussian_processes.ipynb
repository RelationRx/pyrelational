{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing an active learning method with Gaussian Processes\n",
    "\n",
    "This notebook illustrates how to use pyrelational to query new data points to fit a Gaussian Process to a non-linear function. The GP model uses a RBF kernel and it is created using gpytorch. The new points are queried using a least confidence strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "import lightning.pytorch as pl\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Active Learning package\n",
    "from pyrelational.data_managers import DataManager\n",
    "from pyrelational.model_managers import LightningModelManager\n",
    "from pyrelational.pipeline import Pipeline\n",
    "from pyrelational.strategies.regression import LeastConfidenceStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "In order to simply test our Gaussian Processes, we will create a simple function from which we will sample training, validation and testing points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import lightning.pytorch as pl\n",
    "import math\n",
    "\n",
    "def get_dataset():\n",
    "    pl.seed_everything(0)\n",
    "    class SimpleDataset(Dataset):\n",
    "        def __init__(self):\n",
    "            super(SimpleDataset, self).__init__()\n",
    "            x = torch.linspace(0, 4 * math.pi, 140)\n",
    "            y = x * torch.cos(x) + torch.randn(x.size()) * 1e-4 # Function to sample datapoints\n",
    "            self.x = torch.FloatTensor(x)\n",
    "            self.y = torch.FloatTensor(y)\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.x.shape[0]\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.x[idx], self.y[idx]\n",
    "\n",
    "    # Create dataset and splits\n",
    "    ds = SimpleDataset()\n",
    "    train_ds, valid_ds, test_ds = torch.utils.data.random_split(ds, [100, 20, 20])\n",
    "    train_indices = train_ds.indices\n",
    "    valid_indices = valid_ds.indices\n",
    "    test_indices = test_ds.indices\n",
    "\n",
    "    # Create data manager\n",
    "    data_manager = DataManager(\n",
    "        ds,\n",
    "        train_indices=train_indices,\n",
    "        validation_indices=valid_indices,\n",
    "        test_indices=test_indices,\n",
    "        loader_batch_size=\"full\",\n",
    "        loader_shuffle=False,\n",
    "    )\n",
    "    return data_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using [GPytorch](https://gpytorch.ai/) to easily compute GPs implemented in PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyL wrapper for GPytorch model\n",
    "class PyLWrapper(pl.LightningModule):\n",
    "    def __init__(self, train_x, train_y):\n",
    "        super(PyLWrapper, self).__init__()\n",
    "        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        self.gpmodel = ExactGPModel(train_x, train_y, self.likelihood)\n",
    "        self.criterion = gpytorch.mlls.ExactMarginalLogLikelihood(\n",
    "            self.likelihood, self.gpmodel\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gpmodel(x)\n",
    "\n",
    "    def generic_step(self, batch):\n",
    "        x, y = batch\n",
    "        x = self(x)\n",
    "        loss = -self.criterion(x, y)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.generic_step(batch)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.generic_step(batch)\n",
    "        self.log(\"loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.generic_step(batch)\n",
    "        self.log(\"test_loss\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.gpmodel.parameters(), lr=0.1)\n",
    "\n",
    "from pyrelational.model_managers import LightningModelManager\n",
    "\n",
    "# Subclass LightningModel to handle GPytorch\n",
    "class GPLightningModel(LightningModelManager):\n",
    "    def __init__(self, model_class, model_config, trainer_config):\n",
    "        super(GPLightningModel, self).__init__(\n",
    "            model_class, model_config, trainer_config\n",
    "        )\n",
    "\n",
    "    def _init_model(self, train_loader):\n",
    "        for train_x, train_y in train_loader:\n",
    "            return self.model_class(\n",
    "                train_x=train_x, train_y=train_y, **self.model_config\n",
    "            )\n",
    "\n",
    "    def train(self, train_loader, valid_loader):\n",
    "        trainer, ckpt_callback = self.init_trainer()\n",
    "        model = self._init_model(train_loader)\n",
    "        trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=valid_loader)\n",
    "        if valid_loader is not None:\n",
    "            model.load_state_dict(\n",
    "                torch.load(ckpt_callback.best_model_path)[\"state_dict\"]\n",
    "            )\n",
    "\n",
    "        self._current_model = model\n",
    "\n",
    "    def __call__(self, loader):\n",
    "        with torch.no_grad():\n",
    "            self._current_model.gpmodel.eval()\n",
    "            for x, y in loader:\n",
    "                return self._current_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to see the evolution of the predictions with the number of labelled data, obtained with a `Least confidence` strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model, datamanager and strategy are all that is needed to create a pipeline\n",
    "model_manager = GPLightningModel(model_class=PyLWrapper, model_config={}, trainer_config={\"epochs\": 1})\n",
    "data_manager = get_dataset()\n",
    "strategy = LeastConfidenceStrategy()\n",
    "pipeline = Pipeline(data_manager=data_manager, model_manager=model_manager, strategy=strategy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try the `LeastConfidenceStrategy`, annotating each time 10 more data points, until the full training dataset is used. We will later plot the uncertainty of the prediction obtained with the testset and the unlabelled data.\n",
    "\n",
    "Note that labelled and unlabelled data can be obtained using `pipeline.u_indices` and `pipeline.l_indices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "\n",
    "def active_learning_run(pipeline, num_annotate=10):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pipeline: pyrelational pipeline \n",
    "        num_annotate: number of data points to annotate at each iteration\n",
    "    Return:\n",
    "        index_dict: the updated indices of the training,\n",
    "                    unlabelled and query data point after each update\n",
    "        distributions: the prediction obtained with the GP for \n",
    "                    all the (unlabelled + testset) datapoints\n",
    "    \"\"\"\n",
    "    index_dict = {'iteration':[]}\n",
    "    distributions = []\n",
    "    test_distribution = [] \n",
    "\n",
    "    # run until there is no unlabelled data left\n",
    "    while len(pipeline.u_indices) > 0:\n",
    "            print('percentage of labelled data: {:.2f}'.format(pipeline.percentage_labelled))      \n",
    "            # New data to be annotated based on the active learning strategy\n",
    "            to_annotate = pipeline.step(num_annotate)\n",
    "\n",
    "            # get the predictions on the unlabelled data \n",
    "            unlabelled_ids = pipeline.u_indices\n",
    "            labelled_ids = pipeline.l_indices\n",
    "            predictions = pipeline.model_manager(pipeline.u_loader)\n",
    "            distributions.append(predictions) \n",
    "\n",
    "            # get prediction on the test set\n",
    "            test_predictions = pipeline.model_manager(pipeline.test_loader)\n",
    "            test_distribution.append(test_predictions)\n",
    "\n",
    "            # Query new data and update the data manager \n",
    "            pipeline.query(indices=to_annotate)\n",
    "\n",
    "            # append index dictionary for plots\n",
    "            index_dict['iteration'].append(\n",
    "                {'unlabelled': unlabelled_ids, # unlabelled data\n",
    "                'labelled': labelled_ids, # labelled data\n",
    "                'query': to_annotate, # data to be annotated in the next iteration\n",
    "                'test': test_distribution # test data\n",
    "                 }) \n",
    "\n",
    "    return index_dict, distributions, test_distribution\n",
    "\n",
    "# run the active learning pipeline\n",
    "index_dict, distributions, test_distribution = active_learning_run(pipeline, num_annotate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "\n",
    "\n",
    "def plot_gaussian_process(data_manager, distributions, test_distribution, index_dict):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data_manager: data manager used for the model\n",
    "        distributions: distribution obtained using the GP model for the \n",
    "                       all the (unlabelled + testset) data points\n",
    "        index_dict: return the indices of the training, query and \n",
    "                    unlabelled datasets, that are updated with the Oracle\n",
    "    \"\"\"\n",
    "\n",
    "    # true function\n",
    "    func = lambda x: x * torch.cos(x)\n",
    "    \n",
    "    # labelled, unlabelled and queried indices updated with the active learning strategy\n",
    "    labelled_ids = index_dict['labelled']\n",
    "    unlabelled_ids = index_dict['unlabelled']\n",
    "    query_ids = index_dict['query']\n",
    "\n",
    "    # getting datasets\n",
    "    dataset = data_manager.dataset\n",
    "    labelled_x, labelled_y = dataset[labelled_ids]\n",
    "    unlabelled_x, unlabelled_y = dataset[unlabelled_ids]\n",
    "    test_x, test_y = dataset[data_manager.test_indices]\n",
    "\n",
    "    # reorder indices for matplotlib\n",
    "    unlabelled_x, unlabelled_ids = torch.sort(unlabelled_x.detach())\n",
    "\n",
    "    # # get mean and covariance of prediction\n",
    "    unlabelled_loc = distributions.loc\n",
    "    unlabelled_loc = distributions.loc[unlabelled_ids]\n",
    "    unlabelled_std = distributions.stddev[unlabelled_ids]\n",
    "    \n",
    "    # plot figure\n",
    "    ax = plt.axes()\n",
    "    axis_x = torch.linspace(0, 4 * math.pi, 100)\n",
    "    ax.plot(axis_x, func(axis_x), color='lightsteelblue')\n",
    "    ax.plot(labelled_x, labelled_y, 'o', markersize=4, label='observations', color='royalblue', alpha=0.4)\n",
    "    ax.plot(unlabelled_x, unlabelled_loc, 'o', markersize=4, label='predictions', color='orange', alpha=0.4)\n",
    "    if query_ids is not None:\n",
    "        query_x, query_y = dataset[query_ids]\n",
    "        ax.plot(query_x, query_y, 'o', markersize=2, label='query', color='red')\n",
    "    err = 1.96 * unlabelled_std\n",
    "    ax.fill_between(unlabelled_x, unlabelled_loc - err, unlabelled_loc + err, color='papayawhip', alpha=0.8)\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(index_dict['iteration'])):\n",
    "    plot_gaussian_process(data_manager, distributions[i], test_distribution[i], index_dict['iteration'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyrelational",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d10ffd8b003d4f80f72fca330604dfa8c3494f80522397a91d942c40630050f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
